{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function End to End - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 행렬과 벡터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참조 링크 :\n",
    "\n",
    "1. https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.matrix.html\n",
    "1. https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.linalg.det.html\n",
    "1. https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.linalg.solve.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.matrix('1, 4, 2, 0; 9, 5, 0, 0; 4, 0, 2, 4; 6, 1, 8, 3')\n",
    "x = np.matrix('1;2;3;4')\n",
    "print(\"A:\\n\",A)\n",
    "print(\"x:\\n\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1, 4, 2, 0], [9, 5, 0, 0], [4, 0, 2, 4], [6, 1, 8, 3]])\n",
    "x = np.array([1,2,3,4])\n",
    "print(\"A:\\n\",A)\n",
    "print(\"x:\\n\",x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 01-1 (Matrix-Vector Multiplication)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "1 & 4 & 2 & 0 \\\\\n",
    "9 & 5 & 0 & 0 \\\\\n",
    "4 & 0 & 2 & 4 \\\\\n",
    "6 & 1 & 8 & 3\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "1\\\\2\\\\3\\\\4\n",
    "\\end{bmatrix}=?\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO For문으로 행렬 계산하기 - Matrix 버전\n",
    "A = np.matrix('1, 4, 2, 0; 9, 5, 0, 0; 4, 0, 2, 4; 6, 1, 8, 3')\n",
    "x = np.matrix('1;2;3;4')\n",
    "b = np.zeros((4,1))\n",
    "n = 4\n",
    "for i in range(0, n):\n",
    "    val = 0.0\n",
    "    for j in range(0,n):\n",
    "        val += None\n",
    "    b[i] = val\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO For문으로 행렬 계산하기 - Array 버전\n",
    "A = np.array([[1, 4, 2, 0], [9, 5, 0, 0], [4, 0, 2, 4], [6, 1, 8, 3]])\n",
    "x = np.array([1,2,3,4])\n",
    "b = np.array([0,0,0,0])\n",
    "n = 4\n",
    "for i in range(0, n):\n",
    "    val = 0.0\n",
    "    for j in range(0,n):\n",
    "        # TODO 2\n",
    "        val += None\n",
    "    b[i] = val\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 01-2 (Matrix-Vector Multiplication)\n",
    "with `numpy`.\n",
    "$$\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "1 & 4 & 2 & 0 \\\\\n",
    "9 & 5 & 0 & 0 \\\\\n",
    "4 & 0 & 2 & 4 \\\\\n",
    "6 & 1 & 8 & 3\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "1\\\\2\\\\3\\\\4\n",
    "\\end{bmatrix}=?\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO numpy로 행렬계산 matrix 버전\n",
    "A = np.matrix('1, 4, 2, 0; 9, 5, 0, 0; 4, 0, 2, 4; 6, 1, 8, 3')\n",
    "print(\"A:\\n\",A)\n",
    "x = np.matrix('1;2;3;4')\n",
    "print(\"x:\\n\", x)\n",
    "\n",
    "b = A*x\n",
    "print(\"b:\\n\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO numpy로 행렬계산 array 버전\n",
    "A = np.array([[1, 4, 2, 0], [9, 5, 0, 0], [4, 0, 2, 4], [6, 1, 8, 3]])\n",
    "print(\"A:\\n\",A)\n",
    "x = np.array([1,2,3,4])\n",
    "print(\"x:\\n\",x)\n",
    "b = np.array([0,0,0,0])\n",
    "\n",
    "b = np.dot(A,x)\n",
    "print(\"b:\\n\", b)\n",
    "\n",
    "b = np.matmul(A,x)\n",
    "print(\"b:\\n\", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-info\">\n",
    "    \n",
    "**NOTE:**\n",
    "\n",
    "Matrix에서의 * 와 Array에서의 * 연산($\\odot$)은 다름에 유의하자, 아래 예제 참조\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1, 4, 2, 0],[9, 5, 0, 0], [4, 0, 2, 4],[6, 1, 8, 3]])\n",
    "x = np.array([1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(A,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(x,A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 02-1 (Matrix-Vector Multiplication)\n",
    "$$\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "4 & 5 & 2 & 1 \\\\\n",
    "2 & 3 & 8 & 0 \\\\\n",
    "1 & 0 & 7 & 2\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "1\\\\2\\\\3\\\\4\n",
    "\\end{bmatrix}=?\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO for문으로 행렬계산 matrix 버전\n",
    "A = np.matrix('4, 5, 2, 1; 2, 3, 8, 0; 1, 0, 7, 2')\n",
    "x = np.matrix('1;2;3;4')\n",
    "print(\"A*x:\\n\", A*x)\n",
    "m = 3\n",
    "n = 4\n",
    "b = np.zeros((m,1))\n",
    "# TODO 5\n",
    "for i in range(0, m):\n",
    "    val = 0\n",
    "    for j in range(0,n):\n",
    "        val += None\n",
    "    b[i] = val\n",
    "print(\"b:\\n\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO for문으로 행렬계산 Array 버전\n",
    "A = np.array([[4, 5, 2, 1], [2, 3, 8, 0], [1, 0, 7, 2]])\n",
    "x = np.array([1,2,3,4])\n",
    "print(\"np.matmul(A, x):\",np.matmul(A,x))\n",
    "m = 3\n",
    "n = 4\n",
    "b = np.zeros(m)\n",
    "# TODO 5\n",
    "for i in range(0, m):\n",
    "    val = 0\n",
    "    for j in range(0,n):\n",
    "        val += None\n",
    "    b[i] = val\n",
    "print(\"b:\", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 02-2 (Matrix-Vector Multiplication)\n",
    "with `numpy`.\n",
    "$$\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "4 & 5 & 2 & 1 \\\\\n",
    "2 & 3 & 8 & 0 \\\\\n",
    "1 & 0 & 7 & 2\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "1\\\\2\\\\3\\\\4\n",
    "\\end{bmatrix}=?\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.matrix('4, 5, 2, 1; 2, 3, 8, 0; 1, 0, 7, 2')\n",
    "x = np.matrix('1;2;3;4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[4, 5, 2, 1], [2, 3, 8, 0], [1, 0, 7, 2]])\n",
    "x = np.array([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO numpy로 행렬계산 \n",
    "np.dot(A,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO numpy로 행렬계산\n",
    "np.matmul(A,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 집고 넘어갈 용어 -1 (Rank, Axis, Row wise, Colum wise, Reduce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![term](./pics/termConcept.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression - 초식을 깊숙히 익히자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![bengioAlgo](./pics/bengioAlgo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression은 맞추고자 하는  y가 아래와 같은 1차 관측치 들의 합이라는 가정에서 출발한다\n",
    "\n",
    "$$\\Large\\hat{y} = w_0 + w_1x_1 + w_2x_2 + ...$$\n",
    "\n",
    "1. Feature : $x_1, x_2,...$     (주어진 데이터)\n",
    "2. Weight : $w_2, w_2,...$      (구해 내야하는 값, Fix but Unkown) \n",
    "3. Bias(or intercept) : $w_0$   (구해 내야하는 값, Fix but Unkown)\n",
    "4. Predicted Value : $\\hat{y}$  (Feature가 주어졌을 때 y와 유사하면 좋은 값)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그렇다면 주어진 데이터(Feature) 를 기반으로 $\\hat{y}$ 와 $y$ 의 차이를 최소화 하는 Weight를 구하면 되네?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![opt1](./pics/1.png)\n",
    "***\n",
    "![opt2](./pics/2.png)\n",
    "***\n",
    "![opt3](./pics/3.png)\n",
    "***\n",
    "![opt4](./pics/4.png)\n",
    "***\n",
    "![opt5](./pics/5.png)\n",
    "***\n",
    "![opt6](./pics/6.png)\n",
    "***\n",
    "![opt7](./pics/7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그럼 어떻게 loss function을 최소로 하는 Weight를 구하는가? 예제를 통해 알아보자\n",
    "\n",
    "Gradient descent method = steepest descent method = 최적화 문제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1 \n",
    "Loss function이 아래와 같이 주어졌다면 아래의 값을 최소로 만드는 x의 값은?\n",
    "\n",
    "$$\\begin{equation}\n",
    "f(x) = x^2 - 4x + 6\n",
    "\\end{equation}$$\n",
    "\n",
    "x 라고 표현은 되었지만, 실제 예측 식에서는 Weight를 나타내는 w로 나타남에 유의 하자. 항상 우리가 찾고자하는 것은 Weight 이다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**2 - 4*x + 6\n",
    "\n",
    "# f = lambda x: x**2 - 4*x + 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 그림그리기 복습 (-5~5 사이로 설정할 것)\n",
    "NumberOfPoints = 101\n",
    "x = None\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx = f(x)\n",
    "print(fx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO None에 무엇이 들어가면 될까?\n",
    "plt.None\n",
    "plt.grid()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('plot of f(x)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 첫번째 시도 : 그냥 다 해보기(Brute Force)\n",
    "모든 점을 다 계산한 후 그중에 가장 작은 값을 뽑기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xid = np.argmin(fx) #이미 Numpy에 argmin 이라는 함수가 존재함\n",
    "xopt = x[xid]\n",
    "print(xopt, f(xopt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,fx)\n",
    "plt.grid()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('plot of f(x)')\n",
    "\n",
    "plt.plot(xopt, f(xopt), 'xr')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 두번째 시도 : Gradient Descent Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Gradient의 기하학적 의미\n",
    "![gd](./pics/gd_1.png)\n",
    "\n",
    "    1. Gradient 와 같은 방향은 함수가 가장 빠르게 증가하는 방향\n",
    "    1. Gradient 와 반대 방향은 함수가 가장 빠르게 감소하는 방향\n",
    "    1. 1차원 에서는 미분값 = 기울기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![grdExp](./pics/grdExp_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With initial $x^{(0)}$, calculate the following equation :\n",
    "$$\\begin{equation}\n",
    "x^{(k+1)} = x^{(k)} - \\alpha \\nabla f(x^{(k)})\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 현재위치에서 : $x^{(k)}$\n",
    "1. 함수(Loss Function)값이 작아지는 방향을 찾아서 : $- \\alpha \\nabla f(x^{(k)}$ ($\\alpha$는 Tuning Factor)\n",
    "1. 이동하는 : $x^{(k+1)}$\n",
    "1. 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x**2 - 4*x + 6 의 미분값\n",
    "def grad_fx(x):\n",
    "    return 2*x - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TODO Gradient Descent 핵심 알고리즘\n",
    "x0 = 0.\n",
    "MaxIter = 100\n",
    "learning_rate = 0.01\n",
    "print(\"{:<4} {:<20} {:<40}\".format(\"i\", \"x\", \"f(x)\"))\n",
    "for i in range(MaxIter):\n",
    "    x1 = None\n",
    "    print(\"{:<3}: {:<20} {:<40}\".format(i, x1, f(x1)))\n",
    "    x0 = x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate를 바꿔가며 Test 하기 위해 함수로 만들어 보자\n",
    "def steepest_descent(func, grad_func, x0, learning_rate=0.01, MaxIter=10, verbose=True):\n",
    "    paths = []\n",
    "    if verbose:\n",
    "        print(\"{:<5} {:<7} {:<20}\".format(\"i\", \"x\", \"f(x)\"))\n",
    "    for i in range(MaxIter):\n",
    "        x1 = None\n",
    "        if verbose:\n",
    "            print('{0:03d} : {1:4.3f}, {2:4.2E}'.format(i, x1, func(x1)))\n",
    "        None\n",
    "        paths.append(x0)\n",
    "    return(x0, func(x0), paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xopt, fopt, paths = steepest_descent(f, grad_fx, 0.0, learning_rate=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0.5, 2.5, 1000)\n",
    "paths = np.array(paths)\n",
    "plt.plot(x,f(x))\n",
    "plt.grid()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('plot of f(x)')\n",
    "\n",
    "plt.plot(paths, f(paths), 'o-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(f(paths), 'o-')\n",
    "plt.grid()\n",
    "plt.xlabel('i')\n",
    "plt.ylabel('cost')\n",
    "plt.title('plot of cost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xopt, fopt, paths = steepest_descent(f, grad_fx, 1.0, learning_rate=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0.5, 3.5, 1000)\n",
    "paths = np.array(paths)\n",
    "plt.plot(x,f(x))\n",
    "plt.grid()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('plot of f(x)')\n",
    "\n",
    "plt.plot(paths, f(paths), 'o-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(f(paths))\n",
    "plt.grid()\n",
    "plt.xlabel('i')\n",
    "plt.ylabel('cost')\n",
    "plt.title('plot of cost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xopt, fopt, paths = steepest_descent(f, grad_fx, 1.0, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0.5, 3.5, 1000)\n",
    "paths = np.array(paths)\n",
    "plt.plot(x,f(x))\n",
    "plt.grid()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('plot of f(x)')\n",
    "\n",
    "plt.plot(paths, f(paths), 'o-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(f(paths))\n",
    "plt.grid()\n",
    "plt.xlabel('i')\n",
    "plt.ylabel('cost')\n",
    "plt.title('plot of cost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xopt, fopt, paths = steepest_descent(f, grad_fx, 3.0, learning_rate=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0.5, 3.5, 1000)\n",
    "paths = np.array(paths)\n",
    "plt.plot(x,f(x))\n",
    "plt.grid()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('plot of f(x)')\n",
    "\n",
    "plt.plot(paths, f(paths), 'o-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(f(paths))\n",
    "plt.grid()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('cost')\n",
    "plt.title('plot of cost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize #scipy 활용 최적값 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: x**2 - 4*x + 6\n",
    "x0 = 0.\n",
    "minimize(f, x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{equation}\n",
    "\\min_{x,y} (2x + y - 2)^2 + (3x + y - 4)^2 + (x + y + 1)^2\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{equation}\n",
    "\\min_{x_1,x_2} (2x_1 + x_2 - 2)^2 + (3x_1 + x_2 - 4)^2 + (x_1 + x_2 + 1)^2\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{equation}\n",
    "\\min_{w_1,w_0} (2w_1 + w_0 - 2)^2 + (3w_1 + w_0 - 4)^2 + (w_1 + w_0 + 1)^2\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수 표현해 보기\n",
    "f = None\n",
    "x0 = [0,0]\n",
    "res = minimize(f, x0)\n",
    "x,y = res.x[0], res.x[1]\n",
    "print(x,y, res.fun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 02\n",
    "$$\\begin{equation}\n",
    "f(x, y) = (x-2)^2 + (y-2)^2\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin, xmax, xstep = -4.0, 4.0, .25\n",
    "ymin, ymax, ystep = -4.0, 4.0, .25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.meshgrid(np.arange(xmin, xmax + xstep, xstep), np.arange(ymin, ymax + ystep, ystep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x,y : (x-2)**2 + (y-2)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = f(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minima = np.array([2., 2.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(*minima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minima_ = minima.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualize import surf\n",
    "surf(f, x, y, minima=minima_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualize import contour_with_quiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_f_x = lambda x, y: 2 * (x-2)\n",
    "grad_f_y = lambda x, y: 2 * (y-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_with_quiver(f, x, y, grad_f_x, grad_f_y, minima=minima_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Gradient를 Vector로 표현하면?\n",
    "x0 = np.array([-2., -2.])\n",
    "MaxIter = 10\n",
    "learning_rate = .25\n",
    "\n",
    "print(\"{:<4} {:<25} {:<40}\".format(\"i\", \"x\", \"f(x)\"))\n",
    "for i in range(MaxIter):\n",
    "    grad = None\n",
    "    x1 = x0 - learning_rate * grad\n",
    "    fval = f(*x1)    \n",
    "    print(\"{:<3}: {:<25} {:<40}\".format(i, str(x1), fval))\n",
    "    x0 = x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수로 만들어 보기\n",
    "def steepest_descent_2d(func, gradx, grady, x0, MaxIter=10, learning_rate=0.25, verbose=True):\n",
    "    paths = [x0]\n",
    "    fval_paths = [func(x0[0], x0[1])]\n",
    "    if verbose:\n",
    "        print(\"{:<4} {:<25} {:<40}\".format(\"i\", \"x\", \"f(x)\"))\n",
    "    for i in range(MaxIter):\n",
    "        grad = np.array([gradx(*x0), grady(*x0)])\n",
    "        x1 = x0 - learning_rate * grad\n",
    "        fval = func(*x1)\n",
    "        if verbose:\n",
    "            print(\"{:<3}: {:<25} {:<40}\".format(i, str(x1), fval))\n",
    "        x0 = x1\n",
    "        paths.append(x0)\n",
    "        fval_paths.append(fval)\n",
    "    paths = np.array(paths)\n",
    "    paths = np.array(np.matrix(paths).T)\n",
    "    fval_paths = np.array(fval_paths)\n",
    "    return(x0, fval, paths, fval_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([-2., -2.])\n",
    "xopt, fopt, paths, fval_paths = steepest_descent_2d(f, grad_f_x, grad_f_y, x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualize import contour_with_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_with_path(f, x, y, paths, minima=np.array([[2],[2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 03\n",
    "$$\\begin{equation}\n",
    "f(x, y) = 3(x-2)^2 + (y-2)^2\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x,y : 3*(x-2)**2 + (y-2)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_f_x = lambda x, y: 6 * (x-2)\n",
    "grad_f_y = lambda x, y: 2 * (y-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xopt, fopt, paths, fval_paths = steepest_descent_2d(f, grad_f_x, grad_f_y, x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surf(f, x, y, minima=minima_)\n",
    "# contour_with_quiver(f, x, y, grad_f_x, grad_f_y, minima=minima_)\n",
    "contour_with_path(f, x, y, paths, minima=np.array([[2],[2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 지금까지 살펴본 Gradient Descent의 수학적 의미\n",
    "* 수학은 극복의 대상이 아니라 논리적 오류에서 우리를 지켜주는 안전망이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![opt10](./pics/10_1.png)\n",
    "***\n",
    "![opt8](./pics/8.png)\n",
    "***\n",
    "![opt9](./pics/9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descent 계열 알고리즘의 핵심 사항\n",
    "\n",
    "1. Weight의 초기값을 무엇으로 할까?\n",
    "2. Search 방향을 어떻게 정할까?\n",
    "3. Learning Rate를 어떤 값으로할까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descent 계열이라고? 그럼 Descent 계열에 Gradient Descent 말고 또 있니? 많다!\n",
    "\n",
    "1. Weight의 초기값을 무엇으로 할까?\n",
    "2. Search 방향을 어떻게 정할까? $\\to - \\alpha \\nabla^2 f(x^{(k)})^{-1}\\nabla f(x^{(k)})$\n",
    "3. Learning Rate를 어떤 값으로할까?\n",
    "\n",
    "$$\\begin{equation}\n",
    "x^{(k+1)} = x^{(k)} - \\alpha \\nabla^2 f(x^{(k)})^{-1}\\nabla f(x^{(k)})\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 03 로 돌아가서 맛만 보자\n",
    "$$\\begin{equation}\n",
    "f(x, y) = 3(x-2)^2 + (y-2)^2\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin, xmax, xstep = -4.0, 4.0, .25\n",
    "ymin, ymax, ystep = -4.0, 4.0, .25\n",
    "x, y = np.meshgrid(np.arange(xmin, xmax + xstep, xstep), np.arange(ymin, ymax + ystep, ystep))\n",
    "f = lambda x,y : 3*(x-2)**2 + (y-2)**2\n",
    "z = f(x, y)\n",
    "minima = np.array([2., 2.])\n",
    "f(*minima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minima_ = minima.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualize import surf\n",
    "surf(f, x, y, minima=minima_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_f_x = lambda x, y: 6 * (x-2)\n",
    "grad_f_y = lambda x, y: 2 * (y-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hessian_f = lambda x, y: np.array([[6.0, 0.0],[0.0, 2.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([-2., -2.])\n",
    "MaxIter = 10\n",
    "learning_rate = 1\n",
    "print(\"{:<4} {:<25} {:<40}\".format(\"i\", \"x\", \"f(x)\"))\n",
    "for i in range(MaxIter):\n",
    "    grad = np.array([grad_f_x(*x0), grad_f_y(*x0)])\n",
    "    hess = hessian_f(*x0)\n",
    "    x1 = x0 - learning_rate * np.linalg.solve(hess, grad)# Key points\n",
    "    fval = f(*x0)\n",
    "    print(\"{:<3} {:<25} {:<40}\".format(i, str(x0), fval))    \n",
    "    x0 = x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수로 만들어보기\n",
    "def newton_descent_2d(func, gradx, grady, hessian, x0, MaxIter=10, learning_rate=1, verbose=True):\n",
    "    paths = [x0]\n",
    "    fval_paths = [func(x0[0], x0[1])]\n",
    "    if verbose:\n",
    "        print(\"{:<4} {:<25} {:<40}\".format(\"i\", \"x\", \"f(x)\"))\n",
    "    for i in range(MaxIter):\n",
    "        grad = np.array([gradx(*x0), grady(*x0)])\n",
    "        hess = hessian(*x0)\n",
    "        x1 = x0 - learning_rate * np.linalg.solve(hess, grad)\n",
    "        fval = func(*x0)\n",
    "        if verbose:\n",
    "            print(\"{:<3} {:<25} {:<40}\".format(i, str(x0), fval))\n",
    "        x0 = x1\n",
    "        paths.append(x0)\n",
    "        fval_paths.append(fval)\n",
    "    paths = np.array(paths)\n",
    "    paths = np.array(np.matrix(paths).T)\n",
    "    fval_paths = np.array(fval_paths)\n",
    "    return(x0, fval, paths, fval_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([-2., -2.])\n",
    "xopt, fopt, paths, fval_paths = newton_descent_2d(f, grad_f_x, grad_f_y, hessian_f, x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualize import contour_with_path\n",
    "contour_with_path(f, x, y, paths, minima=np.array([[2],[2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descent - Newton Method 의 수학적 의미\n",
    "* 수학은 극복의 대상이 아니라 논리적 오류에서 우리를 지켜주는 안전망이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![hessian](./pics/hess_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent vs. Newton Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Descent\n",
    "#learning_rate = 1\n",
    "x0 = np.array([-3., -3.])\n",
    "learning_rate = 1\n",
    "xopt, fopt, paths, fval_paths = steepest_descent_2d(f, grad_f_x, grad_f_y, x0, \n",
    "                                                    learning_rate=learning_rate)\n",
    "contour_with_path(f, x, y, paths, minima=np.array([[2],[2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Descent\n",
    "#learning_rate 감소시켜서 보기\n",
    "x0 = np.array([-3., -3.])\n",
    "learning_rate = 0.25\n",
    "xopt, fopt, paths, fval_paths = steepest_descent_2d(f, grad_f_x, grad_f_y, x0, \n",
    "                                                    learning_rate=learning_rate)\n",
    "contour_with_path(f, x, y, paths, minima=np.array([[2],[2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Newton Method\n",
    "x0 = np.array([-2., -3.])\n",
    "learning_rate = 1.0\n",
    "xopt, fopt, paths, fval_paths = newton_descent_2d(f, grad_f_x, grad_f_y, hessian_f, x0,\n",
    "                                                 learning_rate=learning_rate)\n",
    "contour_with_path(f, x, y, paths, minima=np.array([[2],[2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Newton Method\n",
    "x0 = np.array([-2., -3.])\n",
    "learning_rate = 0.5\n",
    "xopt, fopt, paths, fval_paths = newton_descent_2d(f, grad_f_x, grad_f_y, hessian_f, x0,\n",
    "                                                 learning_rate=learning_rate)\n",
    "contour_with_path(f, x, y, paths, minima=np.array([[2],[2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression - 자 이제 해보자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target function\n",
    "f = lambda x: 1.0/3.0 * x + 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-20, 60, 50)\n",
    "fx = f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,fx)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.rand(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(306)\n",
    "y = fx + 10 * np.random.rand(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y, 'o')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주어진 데이터 $(x_i, y_i)$ for $i=1, 50$에 대하여 아래 loss function을 최소화하는 $a$와 $b$를 구하시오.\n",
    "$$\\begin{equation}\n",
    "\\min_{a,b} \\sum_{i=1}^{50}|ax_i+b - y_i|^2\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Define loss function\n",
    "1. Define grad function\n",
    "1. (Optional) Define hess function\n",
    "1. Tunning Parameter\n",
    "    1. learning_rate\n",
    "    1. MaxIter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Gradient Descent 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(a, b):\n",
    "    return np.sum( (a * x + b - y)**2 )\n",
    "\n",
    "def grad_a(a, b):\n",
    "    return np.sum( 2 * x * (a * x + b - y) )\n",
    "\n",
    "def grad_b(a, b):\n",
    "    return np.sum( 2 * (a * x + b - y) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_0 = -0.5\n",
    "b_0 = 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "w0 = np.array([a_0, b_0])\n",
    "wopt, _, _, _ = steepest_descent_2d(loss, grad_a, grad_b, w0, \n",
    "                                    verbose=False, learning_rate=1E-5, MaxIter=4000)\n",
    "print(wopt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_opt = wopt[0]\n",
    "b_opt = wopt[1]\n",
    "\n",
    "plt.plot(x,y, 'o')\n",
    "plt.grid()\n",
    "plt.plot(x, a_opt * x + b_opt, 'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Newton 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hess_loss(a, b):\n",
    "    return np.array([[np.sum(2 * x * x), np.sum(2 * x )],[np.sum(2 * x ),2 * 50.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wopt, _, _, _ = newton_descent_2d(loss, grad_a, grad_b, hess_loss, w0, \n",
    "                                    verbose=False, learning_rate=1, MaxIter=1)\n",
    "print(wopt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_opt = wopt[0]\n",
    "b_opt = wopt[1]\n",
    "\n",
    "plt.plot(x,y, 'o')\n",
    "plt.grid()\n",
    "plt.plot(x, a_opt * x + b_opt, 'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Scipy 모듈 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고 : https://docs.scipy.org/doc/scipy/reference/tutorial/optimize.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(w):\n",
    "    return np.sum( (w[0] * x + w[1] - y)**2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "res = minimize(loss, w0) # scipy가 알아서 method를 고름\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_opt = res.x[0]\n",
    "b_opt = res.x[1]\n",
    "\n",
    "plt.plot(x,y, 'o')\n",
    "plt.grid()\n",
    "plt.plot(x, a_opt * x + b_opt, 'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = minimize(loss, w0, method='CG') # method를 정해줌\n",
    "print(res)\n",
    "\n",
    "a_opt = res.x[0]\n",
    "b_opt = res.x[1]\n",
    "\n",
    "plt.plot(x,y, 'o')\n",
    "plt.grid()\n",
    "plt.plot(x, a_opt * x + b_opt, 'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Scikit Learn 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 참고 : https://scikit-learn.org/stable/modules/linear_model.html\n",
    "* Scikit은 Descent 계열이 아닌 Normal Equation 사용(singular value decomposition으로 Inverse Matrix를 구함)\n",
    "* Normal Equation 참고 : https://daeson.tistory.com/172"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg = LinearRegression().fit(x.reshape(-1,1), y.reshape(-1, 1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.score(x.reshape(-1,1), y.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reg.coef_, reg.intercept_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.predict(np.array([[10]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이걸 안다고 도움이 될까? - ML 기반 Calibration의 Project 과정 소개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 오늘의 과제 - 2차 함수 모델 fiiting하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data 생성\n",
    "\n",
    "$$\\begin{equation}\n",
    "\\ f(x) = w_0 + w_1x + w_2x^2\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function\n",
    "f = lambda x: x**2 + 1.0/3.0 * x + 5.0\n",
    "x = np.linspace(-20, 60, 50)\n",
    "fx = f(x)\n",
    "plt.plot(x,fx)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "y = fx + 500 * np.random.rand(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y, 'o')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. scipy.optimize 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{equation}\n",
    "loss(w) = \\frac{1}{N}\\sum_{i=1}^N |w_0 x_i^2 + w_1x_i + w_2 - y_i|^2\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Steepest Descent 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{equation}\n",
    "loss(w) = \\frac{1}{N}\\sum_{i=1}^N |w_0 x_i^2 + w_1x_i + w_2 - y_i|^2\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{equation}\n",
    "\\nabla loss(w) =\n",
    "\\frac{2}{N}\\sum_{i=1}^N\n",
    "(w_0 x_i^2 + w_1x_i + w_2 - y_i)\n",
    "\\begin{bmatrix}\n",
    "x_i^2\\\\\n",
    "x_i\\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Newton method 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{equation}\n",
    "loss(w) = \\frac{1}{N}\\sum_{i=1}^N |w_0 x_i^2 + w_1x_i + w_2 - y_i|^2\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{equation}\n",
    "\\nabla loss(w) =\n",
    "\\frac{2}{N}\\sum_{i=1}^N\n",
    "(w_0 x_i^2 + w_1x_i + w_2 - y_i)\n",
    "\\begin{bmatrix}\n",
    "x_i^2\\\\\n",
    "x_i\\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{equation}\n",
    "\\nabla^2 loss(w) =\n",
    "\\frac{2}{N}\\sum_{i=1}^N\n",
    "\\begin{bmatrix}\n",
    "x_i^4 & x_i^3 & x_i^2\\\\\n",
    "x_i^3 & x_i^2 & x_i\\\\\n",
    "x_i^2 & x_i & 1\n",
    "\\end{bmatrix}\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 3가지 방법 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(wopt), loss(w_gd), loss(w_nt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(3, figsize=(15,5))\n",
    "plt.subplot(131)\n",
    "y_pred = wopt[0] * x ** 2 + wopt[1] * x + wopt[2]\n",
    "plt.plot(x,y, 'o')\n",
    "plt.plot(x,y_pred, 'r-')\n",
    "plt.grid()\n",
    "plt.title('scipy.optimize.minimize : {0}'.format(loss(wopt)))\n",
    "\n",
    "plt.subplot(132)\n",
    "y_pred = w_gd[0] * x ** 2 + w_gd[1] * x + w_gd[2]\n",
    "plt.plot(x,y, 'o')\n",
    "plt.plot(x,y_pred, 'r-')\n",
    "plt.grid()\n",
    "plt.title('steepest gradient : {0}'.format(loss(w_gd)))\n",
    "\n",
    "plt.subplot(133)\n",
    "y_pred = w_nt[0] * x ** 2 + w_nt[1] * x + w_nt[2]\n",
    "plt.plot(x,y, 'o')\n",
    "plt.plot(x,y_pred, 'r-')\n",
    "plt.grid()\n",
    "plt.title('Newton Method : {0}'.format(loss(w_nt)))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
