{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function End to End - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function 자체를 바꾸는 Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from helper import gradient_descent # 파일로 제공된 helper.py에서 GD를 부른다\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "f = lambda x: 0.3 * x + 5.0 # Target function\n",
    "x_train = np.linspace(-20, 60, N)\n",
    "np.random.seed(313)\n",
    "y_train = f(x_train) + 10 * np.random.rand(len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_train,y_train, 'o')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling 안하고 regression\n",
    "\n",
    "$$\\begin{equation}\n",
    "\\min_w loss(w;x,y)\n",
    "\\end{equation}$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\begin{equation}\n",
    "loss(w;x,y) = \\frac{1}{N}\\sum_{i=1}^N |w_0x_i + w_1 - y_i|^2\n",
    "\\end{equation}$$\n",
    "\n",
    "$$\\begin{equation}\n",
    "\\nabla loss(w) = \\frac{2}{N}\\sum_{i=1}^N (w_0x_i + w_1 - y_i)\n",
    "\\begin{bmatrix}\n",
    "x_i\\\\1\\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO grad_loss 함수작성하기\n",
    "def loss(w, x_list, y_list):\n",
    "    N = len(x_list)\n",
    "    val = 0.0\n",
    "    for i in range(N):\n",
    "        val += None\n",
    "    return val\n",
    "\n",
    "def grad_loss(w, x_list, y_list):\n",
    "    dim = len(w)\n",
    "    N = len(x_list)\n",
    "    val = np.array([0.0, 0.0])\n",
    "    for i in range(N):\n",
    "        er =  None\n",
    "        val += 2.0 * er * np.array([x_list[i], 1.0]) / N\n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss function의 input parameter 추가\n",
    "1. 지금까지는 loss 함수에서 `w`(혹은 a,b)만 input으로 받고, `data`는 글로벌 변수로 사용했습니다.\n",
    "1. 관찰해보면 `loss(w)`은 `data`(`x_train`, `y_train`)에 따라서  변하게 됩니다.\n",
    "1. 그러므로 `loss(w, x_set, y_set)`이 더 정확한 loss function의 표현입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W0 = np.linspace(-100, 100, 101)\n",
    "W1 = np.linspace(-100, 100, 101)\n",
    "W0, W1 = np.meshgrid(W0,W1)\n",
    "LOSSW = W0 * 0\n",
    "for i in range(W0.shape[0]):\n",
    "    for j in range(W0.shape[1]):\n",
    "        wij = np.array([W0[i,j], W1[i,j]])\n",
    "        LOSSW[i,j] = loss(wij, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = np.array([-5.0, -5.0])\n",
    "w_gd, path_gd = gradient_descent(grad_loss, x_train, y_train, w0, learning_rate=1E-3, MaxIter=1000)\n",
    "print(w_gd, loss(w_gd, x_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = w_gd[0] * x_train + w_gd[1]\n",
    "plt.plot(x_train,y_train, 'o')\n",
    "plt.plot(x_train, y_pred, '-r')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = np.array([-50.0, -50.0])\n",
    "w_gd, path_gd = gradient_descent(grad_loss, x_train, y_train, w0, learning_rate=1E-3, MaxIter=1000)\n",
    "print(w_gd, loss(w_gd, x_train, y_train))\n",
    "\n",
    "paths = path_gd\n",
    "paths = np.array(np.matrix(paths).T)\n",
    "fig, ax = plt.subplots(figsize=(6, 6)) # check\n",
    "\n",
    "ax.contour(W0, W1, LOSSW, cmap=plt.cm.jet, levels=np.linspace(0, max(LOSSW.flatten()),30))\n",
    "ax.quiver(paths[0,:-1], paths[1,:-1], paths[0,1:]-paths[0,:-1], paths[1,1:]-paths[1,:-1], scale_units='xy', angles='xy', scale=1, color='k')\n",
    "\n",
    "ax.set_xlabel('$w_0$')\n",
    "ax.set_ylabel('$w_1$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "\n",
    "$$\n",
    "\\hat{x} = \\frac{x - x_{min}}{x_{max} - x_{min}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Scaling 작성하기\n",
    "scaled_x_train = None\n",
    "W0 = np.linspace(-100, 100, 101)\n",
    "W1 = np.linspace(-100, 100, 101)\n",
    "W0, W1 = np.meshgrid(W0,W1)\n",
    "LOSSW = W0 * 0\n",
    "LOSSW_Scaled = W0 * 0\n",
    "for i in range(W0.shape[0]):\n",
    "    for j in range(W0.shape[1]):\n",
    "        wij = np.array([W0[i,j], W1[i,j]])\n",
    "        LOSSW[i,j] = loss(wij, x_train, y_train)\n",
    "        LOSSW_Scaled[i,j] = loss(wij, scaled_x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = np.array([-50.0, -50.0])\n",
    "w_gd_sc, path_gd_sc = gradient_descent(grad_loss, scaled_x_train, y_train, w0, learning_rate=.5, MaxIter=100)\n",
    "print(w_gd_sc, loss(w_gd_sc, scaled_x_train, y_train))\n",
    "\n",
    "paths = path_gd_sc\n",
    "paths = np.array(np.matrix(paths).T)\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "ax.contour(W0, W1, LOSSW_Scaled, cmap=plt.cm.jet, levels=np.linspace(0, max(LOSSW_Scaled.flatten()),30))\n",
    "ax.quiver(paths[0,:-1], paths[1,:-1], paths[0,1:]-paths[0,:-1], paths[1,1:]-paths[1,:-1], scale_units='xy', angles='xy', scale=1, color='k')\n",
    "\n",
    "ax.set_xlabel('$w_0$')\n",
    "ax.set_ylabel('$w_1$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = np.array([-50.0, -50.0])\n",
    "w_gd, path_gd = gradient_descent(grad_loss, x_train, y_train, w0, learning_rate=1E-3, MaxIter=1000)\n",
    "print(w_gd, loss(w_gd, x_train, y_train))\n",
    "\n",
    "w_gd_sc, path_gd_sc = gradient_descent(grad_loss, scaled_x_train, y_train, w0, learning_rate=.5, MaxIter=100)\n",
    "print(w_gd_sc, loss(w_gd_sc, scaled_x_train, y_train))\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.subplot(221)\n",
    "paths = path_gd\n",
    "paths = np.array(np.matrix(paths).T)\n",
    "\n",
    "plt.contour(W0, W1, LOSSW, cmap=plt.cm.jet, levels=np.linspace(0, max(LOSSW.flatten()),90))\n",
    "plt.quiver(paths[0,:-1], paths[1,:-1], paths[0,1:]-paths[0,:-1], paths[1,1:]-paths[1,:-1], scale_units='xy', angles='xy', scale=1, color='k')\n",
    "\n",
    "plt.xlabel('$w_0$')\n",
    "plt.ylabel('$w_1$')\n",
    "plt.title('original')\n",
    "\n",
    "plt.subplot(222)\n",
    "paths = path_gd_sc\n",
    "paths = np.array(np.matrix(paths).T)\n",
    "\n",
    "plt.contour(W0, W1, LOSSW_Scaled, cmap=plt.cm.jet, levels=np.linspace(0, max(LOSSW_Scaled.flatten()),90))\n",
    "plt.quiver(paths[0,:-1], paths[1,:-1], paths[0,1:]-paths[0,:-1], paths[1,1:]-paths[1,:-1], scale_units='xy', angles='xy', scale=1, color='k')\n",
    "\n",
    "plt.xlabel('$w_0$')\n",
    "plt.ylabel('$w_1$')\n",
    "plt.title('scaled')\n",
    "\n",
    "plt.subplot(223)\n",
    "y_pred = w_gd[0] * x_train + w_gd[1]\n",
    "plt.plot(x_train, y_train, 'o')\n",
    "plt.plot(x_train, y_pred, '-r')\n",
    "plt.grid()\n",
    "plt.title('original')\n",
    "\n",
    "plt.subplot(224)\n",
    "y_pred = w_gd_sc[0] * scaled_x_train + w_gd_sc[1]\n",
    "plt.plot(scaled_x_train, y_train, 'o')\n",
    "plt.plot(scaled_x_train, y_pred, '-r')\n",
    "plt.grid()\n",
    "plt.title('scaled')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss 함수의 Gradient 계산 한계 -  계산속도 저하와 메모리 용량 문제 발생\n",
    "\n",
    "* Data가 50000개 있다면\n",
    "\n",
    "    - 1차원 선형회귀 문제 : 50,000 x 4bytes = 200,000bytes = 0.2MB\n",
    "    - MNIST 이미지 : 28 x 28 x 50,000 x 4bytes = 156MB\n",
    "    - 저화질 이미지 : 280 x 280 x 50,000 x 4bytes = 15,600MB = 15.6GB\n",
    "    - 중간화질 이미지 : 560 x 560 x 50,000 x 4bytes = 15,600MB = 62.4GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터를 일부만 가지고 Gradient를 추정하면 되지 않을까?\n",
    "\n",
    "1. 장점\n",
    "    * 계산속도 증가, 메모리 용량 감소\n",
    "    * Local Minimum을 빠져나갈 가능성 증가\n",
    "\n",
    "\n",
    "2. 단점\n",
    "    * 부정확한 Gradient 값\n",
    "    * 가장 빠른 Search 방향 설정 불가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sgd](./pics/sgd1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gdgd1](./pics/gdgd.png)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gdgd](./pics/sgdsgd_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Method의 핵심 아이디어\n",
    "1. `x_train`와 `y_train`이 너무 많아서 메모리가 부족하거나 계산이 오래걸리네.\n",
    "1. `x_train`와 `y_train`의 일부만으로 gradient 계산하자.\n",
    "1. RANDOM 하게 섞으면 일부분만으로도 충분히 gradient를 잘 추정할 수 있을 것이다.\n",
    "\n",
    "__STOCHASTIC = RANDOM__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실습 포인트\n",
    "1. `np.random.suffle()`을 사용하여 `x_train`과 `y_train`을 섞기.\n",
    "    - Hint 1\n",
    "```python\n",
    "shuffled_id = np.arange(0, N)\n",
    "np.random.shuffle(shuffled_id)\n",
    "```\n",
    "\n",
    "1. `generate_batches()`를 사용하여 여러개의 batch로 나누기 : `batch_size = 5`\n",
    "```python\n",
    "for x_batch, y_batch in generate_batches(batch_size, x_train, y_train):\n",
    "```\n",
    "1. 각 나눈 batch 들로 Gradient를 계산하고\n",
    "```python\n",
    "grad = grad_loss(w0, x_batch, y_batch)\n",
    "```\n",
    "1. Gradient Decent Method를 적용\n",
    "```python\n",
    "w1 = w0 - learning_rate * grad\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#그림을 자세히 보기 위해 범위만 바꿈.\n",
    "N = 100\n",
    "f = lambda x: 0.3 * x + 5.0 # Target function\n",
    "x_train = np.linspace(-1, 1, N)\n",
    "np.random.seed(313)\n",
    "y_train = f(x_train) + 0.2 * np.random.rand(len(x_train))\n",
    "\n",
    "W0 = np.linspace(-10, 10, 101)\n",
    "W1 = np.linspace(-10, 10, 101)\n",
    "W0, W1 = np.meshgrid(W0,W1)\n",
    "LOSSW = W0 * 0\n",
    "for i in range(W0.shape[0]):\n",
    "    for j in range(W0.shape[1]):\n",
    "        wij = np.array([W0[i,j], W1[i,j]])\n",
    "        LOSSW[i,j] = loss(wij, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Batch generator 만들기\n",
    "def generate_batches(batch_size, features, labels):\n",
    "    \"\"\"\n",
    "    Create batches of features and labels\n",
    "    :param batch_size: The batch size\n",
    "    :param features: List of features\n",
    "    :param labels: List of labels\n",
    "    :return: Batches of (Features, Labels)\n",
    "    \"\"\"\n",
    "    assert len(features) == len(labels) # Test, 여기서 에러나면 실행 안되도록\n",
    "    out_batches = []\n",
    "\n",
    "    sample_size = len(features)\n",
    "    for start_i in range(0, sample_size, batch_size):\n",
    "        end_i = None\n",
    "        batch = [features[start_i:end_i], labels[start_i:end_i]]\n",
    "        out_batches.append(batch)\n",
    "\n",
    "    return out_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_batches = generate_batches(5, x_train, y_train)\n",
    "for x_batch, y_batch in out_batches:\n",
    "    print(x_batch)\n",
    "    print(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO SGD 만들기\n",
    "batch_size = 10\n",
    "path_sgd = []\n",
    "cost_history_sgd = []\n",
    "w0 = np.array([-5.0, -5.0])\n",
    "path_sgd.append(w0)\n",
    "learning_rate = 0.1\n",
    "MaxIter = 100\n",
    "\n",
    "shuffled_id = np.arange(0, N)\n",
    "#TODO1\n",
    "None # Hint1 활용\n",
    "x_train = None\n",
    "y_train = None\n",
    "for i in range(MaxIter):\n",
    "    for x_batch, y_batch in generate_batches(batch_size, None):\n",
    "        grad = None\n",
    "        w1 = None\n",
    "        #TODO4\n",
    "        w0 = w1\n",
    "        path_sgd.append(w1)\n",
    "        cost_history_sgd.append(loss(w0, x_batch, y_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = path_sgd\n",
    "paths = np.array(np.matrix(paths).T)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "ax.contour(W0, W1, LOSSW, cmap=plt.cm.jet, levels=np.linspace(0, max(LOSSW.flatten()),20))\n",
    "ax.quiver(paths[0,:-1], paths[1,:-1], paths[0,1:]-paths[0,:-1], paths[1,1:]-paths[1,:-1], scale_units='xy', angles='xy', scale=1, color='k')\n",
    "\n",
    "ax.set_xlabel('$w_0$')\n",
    "ax.set_ylabel('$w_1$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD와 GD 방법론 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import gradient_descent\n",
    "w0 = np.array([-5.0, -5.0])\n",
    "w_gd, path_gd = gradient_descent(grad_loss, x_train, y_train, w0, learning_rate=0.1, MaxIter=500)\n",
    "print(w_gd, loss(w_gd, x_train, y_train))\n",
    "paths = path_gd\n",
    "paths = np.array(np.matrix(paths).T)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "ax.contour(W0, W1, LOSSW, cmap=plt.cm.jet, levels=np.linspace(0, max(LOSSW.flatten()),30))\n",
    "ax.quiver(paths[0,:-1], paths[1,:-1], paths[0,1:]-paths[0,:-1], paths[1,1:]-paths[1,:-1], scale_units='xy', angles='xy', scale=1, color='k')\n",
    "\n",
    "ax.set_xlabel('$w_0$')\n",
    "ax.set_ylabel('$w_1$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remind : Descent 계열 알고리즘의 핵심 사항\n",
    "\n",
    "1. Weight의 초기값을 무엇으로 할까?\n",
    "2. Search 방향을 어떻게 정할까?\n",
    "3. Learning Rate를 어떤 값으로할까? $\\to$ 이걸 자동으로 최적값을 찾아 주는 건 없나?\n",
    "\n",
    "참조 : http://shuuki4.github.io/deep%20learning/2016/05/20/Gradient-Descent-Algorithm-Overview.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![adap](./pics/adapAlpha.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 이 또한 맛만 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.linspace(-1, 1, N)\n",
    "np.random.seed(313)\n",
    "y_train = f(x_train) + 0.2 * np.random.rand(len(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### learning_rate이 큰 경우\n",
    "`learning_rate=1.5`로 설정하면, loss function이 계속 커짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = np.array([-5.0, -5.0])\n",
    "learning_rate = 1.5\n",
    "MaxIter = 10\n",
    "for i in range(MaxIter):\n",
    "    loss0 = loss(w0, x_train, y_train)\n",
    "    grad = grad_loss(w0, x_train, y_train)\n",
    "    w1 = w0 - learning_rate * grad\n",
    "    print(i, w0, loss0)\n",
    "    w0 = w1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### learning_rate이 작은 경우\n",
    "`learning_rate=0.0001`로 설정하면, loss function이 계속 작아지지만, 너무 느림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = np.array([-5.0, -5.0])\n",
    "learning_rate = 0.0001\n",
    "MaxIter = 10\n",
    "for i in range(MaxIter):\n",
    "    loss0 = loss(w0, x_train, y_train)\n",
    "    grad = grad_loss(w0, x_train, y_train)\n",
    "    w1 = w0 - learning_rate * grad\n",
    "    print(i, w0, loss0)\n",
    "    w0 = w1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 자동으로 learning_rate을 조정하는 아이디어는 없을까?\n",
    "매번 `learning_rate`을 감으로 때려넣지 않고 자동으로 찾아가게 해봅시다.\n",
    "\n",
    "1. `loss(w1,x_train,y_train)`가 `loss(w0,x_train,y_train)`보다 커질때마다, `learning_rate`을 2배씩 작게 하고\n",
    "1. 다시 next position(`w1`)을 업데이트 하고 `loss(w1,x_train,y_train)`을 계산해서 `loss(w0,x_train,y_train)` 값과 비교해봅니다.\n",
    "1. `loss(w1,x_train,y_train) < loss(w0,x_train,y_train)`이 참값이 될때까지, `learning_rate`를 2배씩 줄입니다.\n",
    "\n",
    "\n",
    "__HINT:__\n",
    "```python\n",
    "if loss(w1) > loss(w0): # loss(w1) 값이 loss(w0)보다 크다면\n",
    "    #learning_rate을 줄입니다.\n",
    "else: # loss(w1) 값이 loss(w0)보다 작다면\n",
    "    #업데이트 합니다.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Adaptive GD 만들기\n",
    "w0 = np.array([-5.0, -5.0])\n",
    "learning_rate = 10\n",
    "MaxIter = 10\n",
    "for i in range(MaxIter):\n",
    "    loss0 = loss(w0, x_train, y_train)\n",
    "    grad = grad_loss(w0, x_train, y_train)\n",
    "    w1 = w0 - learning_rate * grad\n",
    "    loss1 = loss(w1, x_train, y_train)\n",
    "    if None:\n",
    "        None\n",
    "    else:\n",
    "        None\n",
    "    print(i, w0, loss0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 집고 넘어갈 용어 -2 (Linear Model, Linear Regression, Feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from scipy.optimize import minimize\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_model(w, x): # Fix But Unknown 모델\n",
    "    assert w.shape[0] == x.shape[0]\n",
    "    return w.dot(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_mse(w, feature_set, label_set):\n",
    "    val = 0.0\n",
    "    for feature, label in zip(feature_set, label_set):\n",
    "        val = val + np.abs(forward_model(w, feature) - label)\n",
    "    val /= len(feature_set)\n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Linear Regression Model\n",
    "$$\n",
    "f(x) = w_0 + w_1x\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(327)\n",
    "x = np.linspace(-1,1,50)\n",
    "y = x + 1.0 + np.random.rand(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, 'o')\n",
    "plt.grid()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features1 = np.array([[1., xval] for xval in x])\n",
    "labels1 = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = np.array([0.7, 1])\n",
    "loss1 = lambda w: loss_mse(w, features1, labels1)\n",
    "res = minimize(loss1, w0)\n",
    "print(res)\n",
    "w1 = res.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction1 = np.dot(features1, w1)\n",
    "plt.plot(x, y, 'o')\n",
    "plt.plot(x, prediction1, '--r')\n",
    "plt.grid()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Cubic Regression Model\n",
    "$$\n",
    "f(x) = w_0 + w_1x + w_2x^2 + w_3x^3\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(327)\n",
    "x = np.linspace(-1,1,50)\n",
    "y = 2*x**3 - .4 * x**2 + .5 * x + 1.0 + 0.5 * np.random.rand(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, 'o')\n",
    "plt.grid()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features3 = np.array([[1, xval, xval**2, xval**3] for xval in x])\n",
    "labels3 = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = np.array([0.0, 0.7, 1, 0.5])\n",
    "loss3 = lambda w: loss_mse(w, features3, labels3)\n",
    "res = minimize(loss3, w0)\n",
    "print(res)\n",
    "w3 = res.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction3 = np.dot(features3, w3)\n",
    "plt.plot(x, y, 'o')\n",
    "plt.plot(x, prediction3, '--r')\n",
    "plt.grid()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Exponential Regression Model\n",
    "$$\\begin{align}\n",
    "f(x) &= e^{w_0x}\\\\\n",
    "g(x) &= \\ln f(x) = w_0x\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(327)\n",
    "x = np.linspace(-1,1,50)\n",
    "y = np.exp(2 * x) + 0.2 * (2* np.random.rand(len(x))-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, 'o')\n",
    "plt.grid()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features4 = np.array([[xval] for xval in x])\n",
    "labels4 = np.log(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = np.array([0.2])\n",
    "loss4 = lambda w: loss_mse(w, features4, labels4)\n",
    "res = minimize(loss4, w0)\n",
    "print(res)\n",
    "w4 = res.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction4 = np.exp(np.dot(features4, w4))\n",
    "plt.plot(x, y, 'o')\n",
    "plt.plot(x, prediction4, '--r')\n",
    "plt.grid()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Sine/cosine Regression\n",
    "$$\n",
    "f(x) = w_0\\cos(\\pi x) + w_1\\sin(\\pi  x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(327)\n",
    "x = np.linspace(-1,1,50)\n",
    "y = 1.0 * np.cos(np.pi * x) + 1.0 * np.sin(np.pi*x) + 2 * np.random.rand(len(x)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, 'o')\n",
    "plt.grid()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 수식 표현 해 보기\n",
    "features5 = None\n",
    "labels5 = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = np.array([0.2, 0.7])\n",
    "loss5 = lambda w: loss_mse(w, features5, labels5)\n",
    "res = minimize(loss5, w0)\n",
    "print(res)\n",
    "w5 = res.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction5 = np.dot(features5, w5)\n",
    "plt.plot(x, y, 'o')\n",
    "plt.plot(x, prediction5, '--r')\n",
    "plt.grid()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-info\">\n",
    "    \n",
    "**NOTE:**\n",
    "\n",
    "여기까지 보신 모든 모델이 Linear Model 입니다!!!!! \n",
    "\n",
    "모델이 $Xw$ 의 형태로 표현되면 Linear Model이라 칭합니다.\n",
    "\n",
    "$X$ : Feature\n",
    "\n",
    "$w$ : Weight\n",
    "\n",
    "그럼 도대체 무엇에 대해 선형이란 것인가? weight에 대해 선형이라는 것입니다.\n",
    "\n",
    "즉, $w_0w_1x + w_2x^2$ 같은 것은 선형 모델이 아닙니다\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![linear](./pics/linear.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "많은 문제를 Linear Model로 풀어낼 수 있습니다. 대강의 과정은 다음과 같습니다. \n",
    "\n",
    "1. 모델의 목표값이 될 변수가 발현될 전체 영역에서 고루 분포되어 있는 데이터를 가지고 있는가?\n",
    "   * 간단히 Histogram으로 Balance를 보거나\n",
    "   * 그렇지 않다면 적절한 시험 계획을 통해 데이터를 확보해야 합니다.\n",
    "   \n",
    "\n",
    "2. 목표값과 관련있다고 생각되는 변수의 전체 상관관계를 파악하여 선형성을 가지는 것으로 보이는 Feature를 선택합니다.(피어슨 상관계수)\n",
    "   * Feature 선정은 당연히 쉽지 않으나, 우리가 푸는 문제의 대부분은 아래와 같다는 가정에서 시작해 봅니다\n",
    "     - 누군가 고민했던 문제거나(논문에서 지배 방정식의 요소 요소를 참조)\n",
    "     - 나도 충분히 고민해왔던 문제(내가 잘 알고 있는 변수 부터 시작한다)\n",
    "     \n",
    "\n",
    "3. 그 후 여러가지 Linear Model을 이용해서 Regression을 해봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification - 초식을 깊숙히 익히자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![binClass](./pics/binClass.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H1, H2, H3중 어떤 모델이 좋은 모델일까?\n",
    "\n",
    "1. H3 보다는 H1, H2가 좋다 : 틀린 갯수가 적으니까\n",
    "2. H1과 H2 중에는? : 갯수만 가지고서는 좋고 나쁨을 비교할 수 없다.$\\to$ Continuous한 평가 방식 필요\n",
    "3. 틀리다 맞다(0,1)가 아니라 어느정도 틀렸다를 나타내고 싶음 $\\to$ 0~1 사이에 각 점을 매핑 $\\to$ 결국 확률로 표현됨\n",
    "4. 0~1 사이 각 점을 매핑 하는 함수 : sigmoid (Activation Function의 한 종류)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight Variation | Bias Variation\n",
    "- | - \n",
    "![sigmoidWeight](./pics/weight_animation.gif)|![sigmoidBias](./pics/bias_animation.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sig3d](./pics/sig3d.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![binNewLine](./pics/binNewLine_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 좋은 모델은 어떻게 구하지? $\\to$ 확률 기반의 Loss function을 정의 하면 되겠네?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification의 Loss function = Cross Entropy\n",
    "$$loss(y, \\hat{y}) = \\frac{1}{N}\\sum_{i=1}^{N}-y_i\\log\\hat{y_i}-(1-y_i)\\log(1-\\hat{y_i})$$\n",
    "\n",
    "where\n",
    "$$\\hat{y^{(i)}} = \\sigma(x^{(i)}W)$$\n",
    "\n",
    "1. Linear Model\n",
    "\n",
    "$$ z = x W =\n",
    "\\begin{bmatrix}\n",
    "1 & x_1 & x_2\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "w_0 \\\\ w_1 \\\\ w_2\n",
    "\\end{bmatrix}=w_0 + w_1 x_1 + w_2x_2\n",
    "$$\n",
    "\n",
    "1. Sigmoid\n",
    "\n",
    "$$ \\sigma(z)=\\frac{1}{1+e^{-z}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 수학적 의미 보다 loss의 감을 잡아보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 간단히 집고 넘어가는 확률의 법칙"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 확률 합의 법칙 : 두개의 사건이 동시에 일어나지 않을 때 각각 독립사건(조건) 확률을 더하면 전체 사건 확률이 된다\n",
    "\n",
    "$$P(ALL)=\\sum{P(X, Y)}=P(X \\cup Y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 확률 곱의 법칙 : 두개의 사건이 동시에 일어날 때 각각 독립사건(조건) 확률을 곱하면 동시 발생 전체 사건 확률이 된다\n",
    "\n",
    "$$P(ALL)=P(Y|X)P(X)=P(X \\cap Y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![probability](./pics/probSimple.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![binPoss](./pics/binPoss_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximum Likelihood\n",
    "\n",
    "    * 드러난 현상 기반으로 관심 대상의 발생 확률을 최대로 만드는 모델(혹은 값)\n",
    "    * 동시에 일어나는 사건이라면 현재 나온 답이 정답일 확률을 모두 곱하는 것이 Maximum Likelihood 임\n",
    "    * 즉, 상기의 Binary Classification 상황에서는 \n",
    "    \n",
    "      Maximum Likelihood = 0.9 x 0.8 x 0.1 x 0.1 x 0.7 x 0.8 = 0.004032\n",
    "      \n",
    "    * 자 이제 Loss Function 인 Cross Entropy와 Maximum Likelihood 의 관계를 알아보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ E = \\frac{1}{N}\\sum_{i=1}^{N}-y_i\\log\\hat{y_i}-(1-y_i)\\log(1-\\hat{y_i})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\small\\frac{1}{6}\\sum_{i=1}^{6}-y_i\\log\\hat{y_i} = -1 \\cdot log(0.9) -1 \\cdot log(0.8) -1 \\cdot log(0.1) -0 \\cdot log(0.9) -0 \\cdot log(0.3) -0 \\cdot log(0.2)$$\n",
    "$$ \\small\\frac{1}{6}\\sum_{i=1}^{6}-(1-y_i)\\log(1-\\hat{y_i}) = -0 \\cdot log(0.1) -0 \\cdot log(0.2) -0 \\cdot log(0.9) -1 \\cdot log(0.1) -1 \\cdot log(0.7) -1 \\cdot log(0.8)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\small6E = -log(0.9) -log(0.8) -log(0.1) -log(0.1) -log(0.7) -log(0.8) = -log(0.9 \\times 0.8 \\times 0.1 \\times 0.1 \\times 0.7 \\times 0.8)$$\n",
    "\n",
    "즉, Cross Entropy는 Maximum Likelihood에 Log를 취한 값이다. 즉 정답이 정답일 확률을 구한것과 다르지 않다.\n",
    "\n",
    "이 확률이 높아지도록 Cross Entropy의 Weight를 조정하면 되는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 예제를 풀어보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter(W_, xy, labels):\n",
    "    for k, color in [(0, 'b'), (1, 'r')]:\n",
    "        idx = labels.flatten() == k\n",
    "        plt.scatter(xy[idx, 0], xy[idx, 1], c=color)\n",
    "\n",
    "    x1 = np.linspace(-.1, 1.1)\n",
    "    x2 = -W_[1] / W_[2] * x1  - W_[0] / W_[2]\n",
    "    plt.plot(x1, x2, '--k')\n",
    "\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.array([-4./5., 3./4., 1.0])\n",
    "\n",
    "np.random.seed(327)\n",
    "xy = np.random.rand(30,2)\n",
    "labels = np.zeros(len(xy))\n",
    "labels[W[0] + W[1] * xy[:,0] + W[2] * xy[:,1] > 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter(W, xy, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1\n",
    "features = np.array([[1.0, x, y] for x, y in xy])\n",
    "features[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W0 = np.array([-0.5, 0.7, 1.8]) # Weight 초기값\n",
    "z = np.matmul(features, W0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter(W0, xy, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\sigma(z)=\\frac{1}{1+e^{-z}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Sigmoid를 lambda로 표현해 보기 Hint. exponential 함수는 np.exp()를 사용\n",
    "sigmoid = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 최종 모델 표현\n",
    "z =  None\n",
    "model = sigmoid(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$loss(y, \\hat{y}) = \\frac{1}{30}\\sum_{i=1}^{30}-y_i\\log\\hat{y_i}-(1-y_i)\\log(1-\\hat{y_i})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y, yhat):\n",
    "    val = 0.0\n",
    "    for yi, yhati in zip(y, yhat):\n",
    "        val = val - yi * np.log(yhati) \\\n",
    "                  - (1. - yi) * np.log(1. - yhati)\n",
    "    return val / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = cross_entropy(labels, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GD를 구현하자 - Cross Entropy의 Gradient\n",
    "\n",
    "1. sigmoid의 미분\n",
    "1. linear model의 미분\n",
    "1. FeedForward Model의 미분(Chian Rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01. sigmoid의 미분\n",
    "$$\\begin{align}\n",
    "\\sigma'(z) &= \\left[\\frac{1}{1+e^{-z}}\\right]'\\\\\n",
    "&= \\left(-\\frac{1}{(1+e^{-z})^2}\\right) \\cdot \\left(-e^{-z}\\right)\\\\\n",
    "&= \\frac{e^{-z}}{(1+e^{-z})^2}\\\\\n",
    "&= \\frac{1}{1+e^{-z}}\\frac{e^{-z}}{1+e^{-z}}\\\\\n",
    "&= \\frac{1}{1+e^{-z}}\\frac{1 + e^{-z} - 1}{1+e^{-z}}\\\\\n",
    "&= \\frac{1}{1+e^{-z}}\\left(1 - \\frac{1}{1+e^{-z}}\\right)\\\\\n",
    "&= \\sigma(z) (1-\\sigma(z))\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_prime = lambda z: sigmoid(z) * (1 - sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02. Linear model의 미분\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial}{\\partial W_j}\\left(xW\\right) &= \\frac{\\partial}{\\partial W_j}\\left(\\sum_{i=1}^d x_iW_i \\right)\\\\\n",
    "&= x_j\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03. FeedForward Model의 미분\n",
    "\n",
    "$$\\begin{align}\n",
    "    \\frac{\\partial}{\\partial W_j}\\hat{y_i} &= \\frac{\\partial}{\\partial W_j}\\sigma(x_iW)\\\\\n",
    "    &= \\frac{\\partial}{\\partial z}\\sigma(z)\\frac{\\partial z}{\\partial W_j}\\\\\n",
    "    &=\\sigma'(z)\\frac{\\partial z}{\\partial W_j}\\\\\n",
    "    &=\\sigma'(z)\\frac{\\partial (x_iW)}{\\partial W_j}\\\\\n",
    "    &=\\sigma(z)(1-\\sigma(z))\\frac{\\partial (x_iW)}{\\partial W_j}\\\\\n",
    "    &=\\sigma(z)(1-\\sigma(z))x_j\\\\\n",
    "    &=\\hat{y_i}(1-\\hat{y_i})x_j\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04. 최종 Gradient 계산 (= Backpropagation)\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{1}{30}\\sum_{i=1}^{30}\\frac{\\partial}{\\partial W_j}\\left(-y_i\\log\\hat{y_i}-(1-y_i)\\log(1-\\hat{y_i})\\right)\n",
    "&=\\frac{1}{30}\\sum_{i=1}^{30}-(y_i-\\hat{y_i}) x_j\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO 최종 Gradient\n",
    "def grad_loss(W_, features, labels):\n",
    "    val = np.zeros_like(W0)\n",
    "    for xi, yi in zip(features, labels):\n",
    "        yhati = None\n",
    "        val += None\n",
    "    return val / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = grad_loss(W0, features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "MaxEpochs = 1000\n",
    "W0_ = W0\n",
    "for epoch in range(MaxEpochs):\n",
    "    # TODO 7\n",
    "    grad = grad_loss(W0_, features, labels)\n",
    "    W1 = W0_ - lr * grad\n",
    "    W0_ = W1\n",
    "W_gd = W0_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cross_entropy(labels, sigmoid(np.dot(features, W0))))\n",
    "plot_scatter(W0, xy, labels) # 초기값 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cross_entropy(labels, sigmoid(np.dot(features, W_gd))))\n",
    "plot_scatter(W_gd, xy, labels) # GD 1000번 후 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import generate_batches\n",
    "batch_size = 10\n",
    "lr = 0.1\n",
    "MaxEpochs = 1000\n",
    "W0_ = W0\n",
    "\n",
    "idx = np.arange(0, len(features))\n",
    "None # idx 섞기\n",
    "shuffled_features = features[idx]\n",
    "shuffled_labels = labels[idx]\n",
    "\n",
    "for epoch in range(MaxEpochs):\n",
    "    for x_batch, y_batch in generate_batches(batch_size, shuffled_features, shuffled_labels):\n",
    "        grad = grad_loss(W0_, x_batch, y_batch)\n",
    "        W1 = W0_ - lr * grad\n",
    "        W0_ = W1\n",
    "W_sgd = W0_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(cross_entropy(labels, sigmoid(np.dot(features, W_sgd))))\n",
    "plot_scatter(W_sgd, xy, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{y^{(i)}} = \\sigma(x^{(i)}W)$$\n",
    "\n",
    "1. Linear Model\n",
    "\n",
    "$$ z = x W =\n",
    "\\begin{bmatrix}\n",
    "1 & x_1 & x_2\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "w_0 \\\\ w_1 \\\\ w_2\n",
    "\\end{bmatrix}=w_0 + w_1 x_1 + w_2x_2\n",
    "$$\n",
    "\n",
    "1. Sigmoid\n",
    "\n",
    "$$ \\sigma(z)=\\frac{1}{1+e^{-z}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 어느 새 우린 Neural Net 기초(Perceptron) 문제를 풀고 있었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![neuralNet](./pics/neuralNet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![frbp](./pics/frbp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification 모델 검증하기 - LIME library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 핵심 : 입력값을 조금 바꿨을 때 모델의 예측값이 크게 바뀌면, 그 변수는 중요한 변수이다.\n",
    "수학적으로 LIME이 블랙 박스 모델을 설명하는 방법은 모델을 해석 가능한 간단한 선형 모델로 근사하는 것입니다.\n",
    "\n",
    "복잡한 모델 전체를 근사하는 것은 어렵지만, 우리가 설명하고 싶은 예측 주변에 대해서만 간단한 모델로 근사하는 것은 매우 쉽습니다.\n",
    "\n",
    "아래 그림처럼 어떤 모델의 변수가 두 개만 있고 데이터를 두 변수를 이용해 파란색과 붉은색 두 클래스로 분류한다고 생각해 봅시다. \n",
    "\n",
    "모델의 decision boundary(빨간색과 파란색이 나뉘는 경계)는 매우 복잡합니다. \n",
    "\n",
    "하지만 아래 그림처럼 우리가 설명하려는 데이터(굵은 빨간 십자가)의 살짝 옆에만 본다면, \n",
    "\n",
    "그 주변만 근사한 선형 함수를 만들어낼 수 있습니다. \n",
    "\n",
    "이 선형 함수는 전체적으로 보면 원래 모델과는 전혀 다르지만, 특정 예시 주위에서는 원래 모델과 비슷하게 행동합니다. \n",
    "\n",
    "이렇게 만든 간단한 선형 함수를 보면 이 예시에서 어떤 변수가 중요한 역할을 하는지 알 수 있죠. \n",
    "\n",
    "이것이 LIME에 Local이라는 말이 붙는 이유입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![lime](./pics/lime.png)\n",
    "\n",
    "참고1 : https://nhlmary3.tistory.com/entry/LIME-Locallly-Interpretable-Modelagnostic-Explanation\n",
    "\n",
    "참고2 : https://blog.dominodatalab.com/shap-lime-python-libraries-part-1-great-explainers-pros-cons/\n",
    "\n",
    "Code : https://github.com/marcotcr/lime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 오늘의 과제 1 - 3GB 파일에서 132번째 줄만 불러 함수 모델 fiiting 및 시간 측정하기(SGD 및 sin, cos 활용)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 오늘의 과제 2 - Circle Classification 하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![circleclass](./pics/circleClass.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import make_circles\n",
    "from matplotlib.pylab import plt\n",
    "from pandas import DataFrame\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter(W_, xy, labels):\n",
    "    for k, color in [(0, 'b'), (1, 'r')]:\n",
    "        idx = labels.flatten() == k\n",
    "        plt.scatter(xy[idx, 0], xy[idx, 1], c=color)\n",
    "\n",
    "    theta = np.linspace(-np.pi, np.pi)\n",
    "    assert -W_[0] / W_[1] >= 0\n",
    "    a = np.sqrt(-W_[0] / W_[1])\n",
    "    b = np.sqrt(-W_[0] / W_[2])\n",
    "    x1 = a * np.cos(theta)\n",
    "    x2 = b * np.sin(theta)\n",
    "    plt.plot(x1, x2, '--k')\n",
    "\n",
    "    plt.grid()\n",
    "    plt.axis('equal')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 2d classification dataset\n",
    "xy, labels = make_circles(n_samples=400, noise=0.1)\n",
    "# scatter plot, dots colored by class value\n",
    "df = DataFrame(dict(x=xy[:,0], y=xy[:,1], label=labels))\n",
    "colors = {0:'red', 1:'blue'}\n",
    "fig, ax = plt.subplots()\n",
    "grouped = df.groupby('label')\n",
    "for key, group in grouped:\n",
    "    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "plt.axis('equal')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boundary가 직선이 아닌 원으로 나오도록 Feature를 설정해야함\n",
    "\n",
    "Hint : 원의 방정식\n",
    "\n",
    "$w_0\\cdot 1 + w_1 x^2 + w_2 y^2 = 0$\n",
    "\n",
    "if $w_0 <0$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
