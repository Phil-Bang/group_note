{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 통계 검정 Frame Work End to End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 통계지식 도대체 어디에 어떻게 쓰는데?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![statRoadMap](./pics/statRoadMap.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-info\">\n",
    "    \n",
    "**NOTE:**\n",
    "Post - hoc\n",
    "\n",
    "One-way ANOVA를 하면 3 개 중에 적어도 하나는 통계적으로 다른지를 판단할 수 있음.\n",
    "그러나, 어떤 쌍 (pair)이 서로 다른지는 알수 없다. \n",
    "\n",
    "ex)\n",
    "\n",
    "A, B, C 세 그룹 중 하나라도 다르다는 One-way Anova 분석 결과가 나왔다면\n",
    "\n",
    "A vs. B\n",
    "\n",
    "B vs. C\n",
    "\n",
    "A vs. C\n",
    "\n",
    "를 비교하고자 할 때, 다시 t-test를 각각 3번씩 할 수도 있으나, \n",
    "post-hoc test를 하면, 한번에 세가지 답을 구할 수 있음.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한단계 확장된 Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![statisticsTree](./pics/statistics_strategy_tree.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정규성 검정(= Parametric assumptions satisfied?, 모수적 방법론?)\n",
    "\n",
    "$$X \\sim \\mathcal{N}(\\mu,\\sigma^{2})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 차량 이용성향 분석 - 자료, Data 배포 X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import scipy.stats as stats\n",
    "import statsmodels.stats.api as sms\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#클러스터링 후 통계적으로 유의하게 분류되었는 지를 따져보는 데 유용하게 쓸 수 있는 코드\n",
    "\n",
    "## 분석 변수 순서대로 for문을 진행\n",
    "### 분석 변수 내 unique한 그룹이 3개 이상 >> 정규성 테스트 진행\n",
    "#### 정규성 테스트 결과 p_value null 이거나 0.05 미만인 경우 >> \n",
    "##### Kruskal-Wallis Test (그룹간 차이가 있는지 검정) 진행하여 p_value가 0.05미만이면 사후검정 (wilcox ranksum), 아닌 경우 NaN 처리\n",
    "#### 정규성 테스트 결과 p_value 0.05 이상인 경우 >> \n",
    "##### bartlett test (그룹간 분산 동일 여부 검정) 진행 후, ANOVA 진행 (그룹간 차이 여부 검정) 하여 p_value가 0.05 미만이면 사후검정 (tukey-hsd), 아닌 경우 NaN 처리\n",
    "\n",
    "### 분석 변수 내 unique한 그룹이 3개 미만 >> 정규성 테스트 진행\n",
    "#### 정규성 테스트 결과 p_value null 이거나 0.05 미만인 경우 >>\n",
    "##### 사후검정 (wilcox ranksum) 진행 (그룹이 3개 이상이 아니므로 그룹간 차이를 따로 확인할 필요 없음)\n",
    "#### 정규성 테스트 결과 p_value 0.05 이상인 경우 >> \n",
    "##### bartlett test (그룹간 분산 동일 여부 검정) 진행 후, 사후검정 (t-test) 진행함\n",
    "\n",
    "for k in range(0, len(varlist)):\n",
    "    temp_data = df3_pd.loc[:, ['avg_start_bat11_bat_soc', varlist[k]]].dropna()\n",
    "    temp_formula = 'avg_start_bat11_bat_soc' + \" ~ \" + \"C(\" + varlist[k] + \")\"\n",
    "\n",
    "    unique_levels = sorted(temp_data[varlist[k]].unique())\n",
    "    n_levels = len(unique_levels)\n",
    "    main.iloc[k, 1] = n_levels\n",
    "\n",
    "    ppp = len(list(itertools.combinations(unique_levels, 2)))\n",
    "\n",
    "    unique_combi = []\n",
    "    unique_combi2 = []\n",
    "\n",
    "    for i in range(n_levels-1):\n",
    "        for j in range((i+1), n_levels):\n",
    "            unique_combi.append(unique_levels[i] + \" vs \" + unique_levels[j])\n",
    "            unique_combi2.append([i, j])\n",
    "\n",
    "    ### more than 3 groups\n",
    "    if n_levels > 2:\n",
    "        ##normality test\n",
    "        warnings.filterwarnings('ignore')\n",
    "\n",
    "        #df = pd.DataFrame(data, columns=['value', 'treatment'])    \n",
    "\n",
    "        # the \"C\" indicates categorical data\n",
    "        model = ols(temp_formula, temp_data).fit()\n",
    "        predictValues = model.predict()\n",
    "        residuals = temp_data.loc[:, 'avg_start_bat11_bat_soc'] - predictValues  # 잔차\n",
    "        nor_pvals = stats.shapiro(residuals)[1]\n",
    "\n",
    "        if nor_pvals is None:\n",
    "            main.iloc[k, 2] = \"Kruskal-Wallis test\"\n",
    "            #kruskal test\n",
    "            y = temp_data.loc[:, 'avg_start_bat11_bat_soc']\n",
    "            x = temp_data.loc[:, varlist[k]]\n",
    "\n",
    "            y = np.array(y)\n",
    "            label, idx = np.unique(x, return_inverse=True)\n",
    "            groups = [y[idx == i] for i, l in enumerate(label)]\n",
    "            H, p = stats.kruskal(*groups)\n",
    "\n",
    "            main.iloc[k, 3] = p\n",
    "\n",
    "            ##post-hoc test\n",
    "            if main.iloc[k, 3] < 0.05:\n",
    "                post_hoc_tb = pd.DataFrame(columns=[\"pair\", \"Diff\", \"Lower CI\", \"Upper CI\"])\n",
    "                post_hoc_tb = post_hoc_tb.assign(pair=unique_combi)\n",
    "                post_hoc_tb = post_hoc_tb.fillna(0)\n",
    "\n",
    "                ##sidak correction (#wilcox ranksum)\n",
    "                for q in range(0, ppp):\n",
    "                    s, mp = stats.mannwhitneyu(groups[unique_combi2[q][0]], groups[unique_combi2[q][1]], alternative='two-sided')\n",
    "\n",
    "                    ct1 = len(groups[unique_combi2[q][0]])  #items in dataset 1\n",
    "                    ct2 = len(groups[unique_combi2[q][1]])  #items in dataset 2\n",
    "                    alpha = 0.05       #95% confidence interval\n",
    "                    N = stats.norm.ppf(1 - alpha/2) # percent point function - inverse of cdf\n",
    "\n",
    "                    # The confidence interval for the difference between the two population\n",
    "                    # medians is derived through these nxm differences.\n",
    "                    diffs = sorted([i-j for i in groups[unique_combi2[q][0]] for j in groups[unique_combi2[q][1]]])\n",
    "\n",
    "                    # For an approximate 100(1-a)% confidence interval first calculate K:\n",
    "                    kv = int(round(ct1*ct2/2 - (N * (ct1*ct2*(ct1+ct2+1)/12)**0.5)))\n",
    "\n",
    "                    # The Kth smallest to the Kth largest of the n x m differences \n",
    "                    # ct1 and ct2 should be > ~20\n",
    "                    CI = (diffs[kv], diffs[len(diffs)-kv])\n",
    "\n",
    "                    post_hoc_tb.iloc[q, 1] = round(np.median(diffs), 5)\n",
    "                    post_hoc_tb.iloc[q, 2] = round(CI[0], 5)\n",
    "                    post_hoc_tb.iloc[q, 3] = round(CI[1], 5)\n",
    "\n",
    "                post_hoc.append(post_hoc_tb)\n",
    "\n",
    "            else:\n",
    "                post_hoc.append(float('NaN'))\n",
    "\n",
    "        elif nor_pvals < 0.05:\n",
    "            main.iloc[k, 2] = \"Kruskal-Wallis test\"\n",
    "            #kruskal test\n",
    "            y = temp_data.loc[:, 'avg_start_bat11_bat_soc']\n",
    "            x = temp_data.loc[:, varlist[k]]\n",
    "\n",
    "            y = np.array(y)\n",
    "            label, idx = np.unique(x, return_inverse=True)\n",
    "            groups = [y[idx == i] for i, l in enumerate(label)]\n",
    "            H, p = stats.kruskal(*groups)\n",
    "\n",
    "            main.iloc[k, 3] = p\n",
    "\n",
    "            ##post-hoc test\n",
    "            if main.iloc[k, 3] < 0.05:\n",
    "                post_hoc_tb = pd.DataFrame(columns=[\"pair\", \"Diff\", \"Lower CI\", \"Upper CI\"])\n",
    "                post_hoc_tb = post_hoc_tb.assign(pair=unique_combi)\n",
    "                post_hoc_tb = post_hoc_tb.fillna(0)\n",
    "\n",
    "                ##sidak correction (#wilcox ranksum)\n",
    "                for q in range(0, ppp):\n",
    "                    s, mp = stats.mannwhitneyu(groups[unique_combi2[q][0]], groups[unique_combi2[q][1]], alternative='two-sided')\n",
    "\n",
    "                    ct1 = len(groups[unique_combi2[q][0]])  #items in dataset 1\n",
    "                    ct2 = len(groups[unique_combi2[q][1]])  #items in dataset 2\n",
    "                    alpha = 0.05       #95% confidence interval\n",
    "                    N = stats.norm.ppf(1 - alpha/2) # percent point function - inverse of cdf\n",
    "\n",
    "                    # The confidence interval for the difference between the two population\n",
    "                    # medians is derived through these nxm differences.\n",
    "                    diffs = sorted([i-j for i in groups[unique_combi2[q][0]] for j in groups[unique_combi2[q][1]]])\n",
    "\n",
    "                    # For an approximate 100(1-a)% confidence interval first calculate K:\n",
    "                    kv = int(round(ct1*ct2/2 - (N * (ct1*ct2*(ct1+ct2+1)/12)**0.5)))\n",
    "\n",
    "                    # The Kth smallest to the Kth largest of the n x m differences \n",
    "                    # ct1 and ct2 should be > ~20\n",
    "                    CI = (diffs[kv], diffs[len(diffs)-kv])\n",
    "\n",
    "                    post_hoc_tb.iloc[q, 1] = round(np.median(diffs), 5)\n",
    "                    post_hoc_tb.iloc[q, 2] = round(CI[0], 5)\n",
    "                    post_hoc_tb.iloc[q, 3] = round(CI[1], 5)\n",
    "\n",
    "                post_hoc.append(post_hoc_tb)\n",
    "\n",
    "            else:\n",
    "                post_hoc.append(float('NaN'))\n",
    "\n",
    "        else:\n",
    "            #homogeneity test\n",
    "            bar_st, bar_p = stats.bartlett(*groups)\n",
    "            var_pvals = bar_p\n",
    "            \n",
    "            ## location test ver2 \n",
    "            if var_pvals < 0.05:\n",
    "                main.iloc[k, 2] = \"ANOVA\"\n",
    "                anova_st, anova_p = stats.f_oneway(*groups)\n",
    "                main.iloc[k, 3] = anova_p\n",
    "                \n",
    "                ## post-hoc test\n",
    "                if main.iloc[k, 3] < 0.05:\n",
    "                    tukey_result = pairwise_tukeyhsd(temp_data['avg_start_bat11_bat_soc'], temp_data[varlist[k]], alpha=0.05)\n",
    "                    tukey_data = tukey_result._results_table.data\n",
    "                    tukey_labels = tukey_data[0]\n",
    "                    del tukey_data[0]\n",
    "                    tukey_pd = pd.DataFrame.from_records(tukey_data, columns=tukey_labels)\n",
    "                    tukey_pd['pair'] = tukey_pd['group1'] + ' vs ' + tukey_pd['group2']\n",
    "                    tukey_pd['Diff'] = round(tukey_pd['meandiff'], 5)\n",
    "                    tukey_pd['Lower CI'] = round(tukey_pd['lower'], 5)\n",
    "                    tukey_pd['Upper CI'] = round(tukey_pd['upper'], 5)\n",
    "                    columns = ['group1', 'group2', 'meandiff', 'lower', 'upper', 'reject']\n",
    "                    tukey_pd.drop(columns, inplace=True, axis=1)\n",
    "                    post_hoc.append(tukey_pd)\n",
    "                else:\n",
    "                    post_hoc.append(float('NaN'))\n",
    "                    \n",
    "            ## location test ver3 \n",
    "            else:\n",
    "                main.iloc[k, 2] = \"ANOVA\"\n",
    "                anova_st, anova_p = stats.f_oneway(*groups)\n",
    "                main.iloc[k, 3] = anova_p\n",
    "\n",
    "                ## post-hoc test\n",
    "                if main.iloc[k, 3] < 0.05:\n",
    "                    tukey_result = pairwise_tukeyhsd(temp_data['avg_start_bat11_bat_soc'], temp_data[varlist[k]], alpha=0.05)\n",
    "                    tukey_data = tukey_result._results_table.data\n",
    "                    tukey_labels = tukey_data[0]\n",
    "                    del tukey_data[0]\n",
    "                    tukey_pd = pd.DataFrame.from_records(tukey_data, columns=tukey_labels)\n",
    "                    tukey_pd['pair'] = tukey_pd['group1'] + ' vs ' + tukey_pd['group2']\n",
    "                    tukey_pd['Diff'] = round(tukey_pd['meandiff'], 5)\n",
    "                    tukey_pd['Lower CI'] = round(tukey_pd['lower'], 5)\n",
    "                    tukey_pd['Upper CI'] = round(tukey_pd['upper'], 5)\n",
    "                    columns = ['group1', 'group2', 'meandiff', 'lower', 'upper', 'reject']\n",
    "                    tukey_pd.drop(columns, inplace=True, axis=1)\n",
    "                    post_hoc.append(tukey_pd)\n",
    "                else:\n",
    "                    post_hoc.append(float('NaN'))\n",
    "                    \n",
    "    ## 2 groups\n",
    "    if n_levels == 2:\n",
    "        #normality test\n",
    "        warnings.filterwarnings('ignore')\n",
    "\n",
    "        #df = pd.DataFrame(data, columns=['value', 'treatment'])    \n",
    "\n",
    "        # the \"C\" indicates categorical data\n",
    "        model = ols(temp_formula, temp_data).fit()\n",
    "        predictValues = model.predict()\n",
    "        residuals = temp_data.loc[:, 'avg_start_bat11_bat_soc'] - predictValues # 잔차\n",
    "        nor_pvals = stats.shapiro(residuals)[1]\n",
    "\n",
    "        if nor_pvals is None:\n",
    "            post_hoc_tb = pd.DataFrame(columns=[\"pair\", \"Diff\", \"Lower CI\", \"Upper CI\"])\n",
    "            post_hoc_tb = post_hoc_tb.assign(pair=unique_combi)\n",
    "            post_hoc_tb = post_hoc_tb.fillna(0)\n",
    "            ##sidak correction (#wilcox ranksum)\n",
    "            for q in range(0, ppp):\n",
    "                main.iloc[k, 2] = \"Wilcoxon rank sum test\"\n",
    "                s, mp = stats.mannwhitneyu(groups[unique_combi2[q][0]], groups[unique_combi2[q][1]], alternative='two-sided')\n",
    "                main.iloc[k, 3] = mp\n",
    "\n",
    "                ct1 = len(groups[unique_combi2[q][0]])  #items in dataset 1\n",
    "                ct2 = len(groups[unique_combi2[q][1]])  #items in dataset 2\n",
    "                alpha = 0.05       #95% confidence interval\n",
    "                N = stats.norm.ppf(1 - alpha/2) # percent point function - inverse of cdf\n",
    "\n",
    "                # The confidence interval for the difference between the two population\n",
    "                # medians is derived through these nxm differences.\n",
    "                diffs = sorted([i-j for i in groups[unique_combi2[q][0]] for j in groups[unique_combi2[q][1]]])\n",
    "\n",
    "                # For an approximate 100(1-a)% confidence interval first calculate K:\n",
    "                kv = int(round(ct1*ct2/2 - (N * (ct1*ct2*(ct1+ct2+1)/12)**0.5)))\n",
    "\n",
    "                # The Kth smallest to the Kth largest of the n x m differences \n",
    "                # ct1 and ct2 should be > ~20\n",
    "                CI = (diffs[kv], diffs[len(diffs)-kv])\n",
    "\n",
    "                post_hoc_tb.iloc[q, 1] = round(np.median(diffs), 5)\n",
    "                post_hoc_tb.iloc[q, 2] = round(CI[0], 5)\n",
    "                post_hoc_tb.iloc[q, 3] = round(CI[1], 5)\n",
    "\n",
    "            post_hoc.append(post_hoc_tb)\n",
    "\n",
    "        elif nor_pvals < 0.05:\n",
    "            post_hoc_tb = pd.DataFrame(columns=[\"pair\", \"Diff\", \"Lower CI\", \"Upper CI\"])\n",
    "            post_hoc_tb = post_hoc_tb.assign(pair=unique_combi)\n",
    "            post_hoc_tb = post_hoc_tb.fillna(0)\n",
    "\n",
    "            ##sidak correction (#wilcox ranksum)\n",
    "            for q in range(0, ppp):\n",
    "                main.iloc[k, 2] = \"Wilcoxon rank sum test\"\n",
    "                s, mp = stats.mannwhitneyu(groups[unique_combi2[q][0]], groups[unique_combi2[q][1]], alternative='two-sided')\n",
    "                main.iloc[k, 3] = mp\n",
    "\n",
    "                ct1 = len(groups[unique_combi2[q][0]])  #items in dataset 1\n",
    "                ct2 = len(groups[unique_combi2[q][1]])  #items in dataset 2\n",
    "                alpha = 0.05       #95% confidence interval\n",
    "                N = stats.norm.ppf(1 - alpha/2) # percent point function - inverse of cdf\n",
    "\n",
    "                # The confidence interval for the difference between the two population\n",
    "                # medians is derived through these nxm differences.\n",
    "                diffs = sorted([i-j for i in groups[unique_combi2[q][0]] for j in groups[unique_combi2[q][1]]])\n",
    "\n",
    "                # For an approximate 100(1-a)% confidence interval first calculate K:\n",
    "                kv = int(round(ct1*ct2/2 - (N * (ct1*ct2*(ct1+ct2+1)/12)**0.5)))\n",
    "\n",
    "                # The Kth smallest to the Kth largest of the n x m differences \n",
    "                # ct1 and ct2 should be > ~20\n",
    "                CI = (diffs[kv], diffs[len(diffs)-kv])\n",
    "\n",
    "                post_hoc_tb.iloc[q, 1] = round(np.median(diffs), 5)\n",
    "                post_hoc_tb.iloc[q, 2] = round(CI[0], 5)\n",
    "                post_hoc_tb.iloc[q, 3] = round(CI[1], 5)\n",
    "\n",
    "            post_hoc.append(post_hoc_tb)\n",
    "\n",
    "        else:\n",
    "            #homogeneity test\n",
    "            bar_st, bar_p = stats.bartlett(*groups)\n",
    "            var_pvals = bar_p\n",
    "\n",
    "            post_hoc_tb = pd.DataFrame(columns=[\"pair\", \"Diff\", \"Lower CI\", \"Upper CI\"])\n",
    "            post_hoc_tb = post_hoc_tb.assign(pair=unique_combi)\n",
    "            post_hoc_tb = post_hoc_tb.fillna(0)\n",
    "\n",
    "            ## location test ver2 \n",
    "            if var_pvals < 0.05:\n",
    "                main.iloc[k, 2] = \"t-test with Satterthwaite correction\"\n",
    "                for q in range(0, ppp):\n",
    "\n",
    "                    t_st, t_p = stats.ttest_ind(groups[unique_combi2[q][0]], groups[unique_combi2[q][1]], equal_var=False)\n",
    "                    main.iloc[k, 3] = t_p\n",
    "\n",
    "                    t_estimate = np.mean(groups[unique_combi2[q][0]]) - np.mean(groups[unique_combi2[q][1]])\n",
    "                    cm = sms.CompareMeans(sms.DescrStatsW(groups[unique_combi2[q][0]]), sms.DescrStatsW(groups[unique_combi2[q][1]]))\n",
    "                    t_CI = cm.tconfint_diff(usevar='unequal')\n",
    "\n",
    "                    post_hoc_tb.iloc[q, 1] = round(t_estimate, 5)\n",
    "                    post_hoc_tb.iloc[q, 2] = round(t_CI[0], 5)\n",
    "                    post_hoc_tb.iloc[q, 3] = round(t_CI[1], 5)\n",
    "\n",
    "                post_hoc.append(post_hoc_tb)\n",
    "\n",
    "            ## location test ver3\n",
    "            else:\n",
    "                main.iloc[k, 2] = \"t-test\"\n",
    "                for q in range(0, ppp):\n",
    "\n",
    "                    t_st, t_p = stats.ttest_ind(groups[unique_combi2[q][0]], groups[unique_combi2[q][1]], equal_var=True)\n",
    "                    main.iloc[k, 3] = t_p\n",
    "\n",
    "                    t_estimate = np.mean(groups[unique_combi2[q][0]]) - np.mean(groups[unique_combi2[q][1]])\n",
    "                    cm = sms.CompareMeans(sms.DescrStatsW(groups[unique_combi2[q][0]]), sms.DescrStatsW(groups[unique_combi2[q][1]]))\n",
    "                    t_CI = cm.tconfint_diff(usevar='pooled')\n",
    "\n",
    "                    post_hoc_tb.iloc[q, 1] = round(t_estimate, 5)\n",
    "                    post_hoc_tb.iloc[q, 2] = round(t_CI[0], 5)\n",
    "                    post_hoc_tb.iloc[q, 3] = round(t_CI[1], 5)\n",
    "\n",
    "                post_hoc.append(post_hoc_tb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 차량 기능 연속 사용성 분석(Lift 개념 사용) - 자료, 데이터 배포 X"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
